import { Page } from "puppeteer";
import { JobListingDto } from "../../dto/JobListingDto";
import { getScraperConfig } from "./getScraperConfig";

export const createPaginationScraper = (siteName: string, searchKeyword: string, maxPages: number) => {
    const config = getScraperConfig(siteName);

    if (!config) {
        throw new Error(`âŒ [ì˜¤ë¥˜] ${siteName} í¬ë¡¤ëŸ¬ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
    }

    // âœ… í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜
    const scrape = async (page: Page): Promise<JobListingDto[]> => {
        console.log(`ğŸ”„ ${config.siteName} í¬ë¡¤ë§ ì‹œì‘...`);

        let jobListings: JobListingDto[] = [];

        for (let pageNum = 1; pageNum <= maxPages; pageNum++) {
            console.log(`ğŸ”„ ${config.siteName} - ${pageNum} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...`);

            const pageUrl = config.searchUrl(searchKeyword, pageNum);
            await page.goto(pageUrl, { waitUntil: "networkidle2" });

            // âœ… íŠ¹ì • ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            await page.waitForSelector(config.listSelector, { timeout: 10000 });

            // âœ… í˜„ì¬ í˜ì´ì§€ì˜ ê³µê³  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            const newJobs = await config.extractJobListings(page);

            // âœ… ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°°ì—´ì— ì¶”ê°€
            jobListings = jobListings.concat(newJobs);

            // âœ… 2ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ (ì„œë²„ ë¶€ë‹´ ë°©ì§€)
            await new Promise(resolve => setTimeout(resolve, 2000));
        }

        console.log(`âœ… ${config.siteName}ì—ì„œ ${jobListings.length}ê°œ ê³µê³  í¬ë¡¤ë§ ì™„ë£Œ!`);
        return jobListings;
    };

    // âœ… siteNameê³¼ scrape í•¨ìˆ˜ë¥¼ í¬í•¨í•œ ê°ì²´ ë°˜í™˜
    return { siteName: config.siteName, scrape };
};
